# Rapport de Laboratoire : Architectures Deep Learning (CNN, R-CNN, ViT)

**Sujet :** Deep Learning Lab 2 - Comparaison d'architectures sur MNIST  
**Auteur :** [Votre Nom / Groupe]  
**Contexte :** Exploration des paradigmes de Convolution, D√©tection d'Objet et M√©canismes d'Attention.

---

## üìå 1. Introduction & Objectifs
L'objectif de ce laboratoire est d'analyser le comportement de diff√©rentes architectures de r√©seaux de neurones sur un probl√®me standard (MNIST). Nous avons cherch√© √† comprendre comment des architectures radicalement diff√©rentes (CNN classique, D√©tecteur d'objets, Transformer) abordent la m√™me t√¢che de classification et quels sont leurs co√ªts respectifs en termes de calcul et de performance.

---

## üèõÔ∏è Partie 1 : Approches Convolutionnelles & D√©tection

### 1.1 Le CNN Standard (Baseline)
**Logique Th√©orique :** Le CNN (Convolutional Neural Network) est l'architecture naturelle pour le traitement d'images. Il utilise l'invariance par translation via des filtres locaux (convolutions) pour extraire des caract√©ristiques hi√©rarchiques (bords -> formes -> chiffres).

**Impl√©mentation :** J'ai con√ßu un mod√®le l√©ger ("from scratch") alternant extraction de features et r√©duction de dimensionnalit√©.

```python
# Extrait de mon architecture CNN
self.conv_layers = nn.Sequential(
    nn.Conv2d(1, 32, kernel_size=3, padding=1), # Feature Map: 28x28
    nn.ReLU(),
    nn.MaxPool2d(2),                            # Downsampling: 14x14
    nn.Conv2d(32, 64, kernel_size=3, padding=1) # Augmentation de la profondeur
)
```

**R√©sultats :** Accuracy : 99.01%  
**Temps :** ~73s

**Analyse :** Convergence extr√™mement rapide. Le mod√®le "sait" naturellement comment traiter l'image gr√¢ce √† l'inductance biais√©e des convolutions.

---

### 1.2 Faster R-CNN (Le D√©fi Technique)
**Logique Th√©orique :** Faster R-CNN est con√ßu pour la d√©tection (trouver o√π est l'objet et ce que c'est). Pour l'appliquer √† MNIST, j'ai d√ª formuler l'hypoth√®se que chaque chiffre est un "objet" √† localiser, m√™me si l'image est centr√©e.

**Impl√©mentation (Le "Hack") :** Le mod√®le n√©cessite des coordonn√©es de bo√Ætes (x1, y1, x2, y2). J'ai cr√©√© un Dataset personnalis√© qui g√©n√®re dynamiquement ces bo√Ætes en d√©tectant les pixels non-nuls du chiffre.

```python
# G√©n√©ration dynamique des Bounding Boxes dans le Dataset
non_zero = torch.nonzero(img_tensor.squeeze())
x_min, x_max = torch.min(non_zero[:, 1]).item(), torch.max(non_zero[:, 1]).item()
# La cible devient une bo√Æte englobante + le label
target["boxes"] = torch.as_tensor([[x_min, y_min, x_max+1, y_max+1]], dtype=torch.float32)
```

**R√©sultats :** Accuracy : 94.80%  
**Temps :** ~778s (Env. 13 min)

**Analyse :** Le mod√®le est 10x plus lent que le CNN. C'est une architecture "Overkill" : le r√©seau perd √©norm√©ment de ressources √† proposer des r√©gions (RPN) pour localiser un objet qui est toujours au centre.

---

### 1.3 Transfer Learning (VGG16 & AlexNet)
**Logique Th√©orique :** Utilisation de mod√®les tr√®s profonds pr√©-entra√Æn√©s sur ImageNet. La contrainte majeure est l'adaptation dimensionnelle (ImageNet = 224x224 RGB vs MNIST = 28x28 Gris).

**Impl√©mentation :** J'ai d√ª upscaler artificiellement les images, ce qui augmente drastiquement la m√©moire requise.

```python
transform_tl = transforms.Compose([
    transforms.Resize((224, 224)),       # Upscaling x8
    transforms.Grayscale(num_output_channels=3), # Adaptation RGB
    transforms.ToTensor()
])
# Freeze des poids pour ne r√©-entra√Æner que la couche finale
for param in vgg16.parameters(): param.requires_grad = False
```

**Analyse :** Bien que fonctionnelle, cette approche est inefficace pour MNIST car l'upscaling cr√©e une redondance de donn√©es massive (64x plus de pixels √† traiter).

---

## üëÅÔ∏è Partie 2 : Vision Transformer (ViT)

### 2.1 Approche "Attention Is All You Need"
**Logique Th√©orique :** Contrairement au CNN qui regarde les pixels voisins, le ViT d√©coupe l'image en "patches" (carr√©s) et utilise le m√©canisme de Self-Attention pour que chaque patch puisse "voir" tous les autres patches instantan√©ment. C'est une approche globale et non locale.

### Impl√©mentation "From Scratch"
J'ai impl√©ment√© le d√©coupage en patches et l'ajout d'embeddings positionnels (car l'attention n'a pas de notion d'ordre spatial).

```python
# D√©coupage de l'image en patches via Convolution
self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=7, stride=7)
# Ajout de l'information de position (apprise)
x = x + self.pos_embed
```

**R√©sultats :** Accuracy : ~97% - 98%

**Analyse :** Le ViT performe √©tonnamment bien pour une impl√©mentation "from scratch". Cependant, il est g√©n√©ralement moins performant que le CNN sur de petits datasets car il manque de "Biais Inductif" (il doit apprendre que les pixels voisins sont corr√©l√©s, alors que le CNN le sait par design).

---

## üìä Partie 3 : Analyse Graphique & Synth√®se

### 3.1 Comparaison de Performance (Accuracy / F1)
Les r√©sultats montrent :
- Le **CNN Standard domine** l√©g√®rement (~99%).
- Le **Faster R-CNN** (~94.8%) souffre de sa complexit√© inutile.
- Le **ViT** (~97-98%) est performant mais demande plus de donn√©es.

### 3.2 Comparaison Temporelle
Les temps d'entra√Ænement montrent une disparit√© massive :
- **CNN : ~73s**
- **Faster R-CNN : ~778s** (10√ó plus lent)
- **ViT : entre les deux**

**Interpr√©tation :** Le co√ªt computationnel du R-CNN (RPN, RoI Align, etc.) est injustifiable pour de la simple classification.

---

## üèÜ Conclusion G√©n√©rale
Ce laboratoire d√©montre que **la complexit√© n'est pas toujours synonyme de performance**.

- Pour des t√¢ches simples (images centr√©es, faible r√©solution) : **le CNN est roi**.
- Pour la d√©tection d'objets multiples : **Faster R-CNN reste n√©cessaire**, malgr√© son co√ªt.
- Pour de grands datasets avec relations globales : **le ViT est l'√©tat de l'art**, mais il est data-hungry.

Ce travail a permis de valider exp√©rimentalement les th√©ories de co√ªt/b√©n√©fice des architectures modernes en Deep Learning.

